{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0naiBGFbpipA"
   },
   "source": [
    "# **Data Streaming using PySpark [CN7030]**\n",
    "\n",
    "**`Dr Amin Karami, UEL Docklands Campus, March 2022`**\n",
    "\n",
    "`E: a.karami@uel.ac.uk`\n",
    "\n",
    "`W: www.aminkarami.com`\n",
    "\n",
    "---\n",
    "\n",
    "**updateStateByKey Operation**:\tIt allows you to maintain arbitrary state while continuously updating it with new information. To use this, you will have to do two steps:\n",
    "\n",
    "[1] Define the state - The state can be an arbitrary data type.\n",
    "\n",
    "[2] Define the state update function - Specify with a function how to update the state using the previous state and the new values from an input stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "08YldFGupg4t"
   },
   "outputs": [],
   "source": [
    "# Load Spark engine\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8zJVMyJgpg4u"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mGzfWI83pg4v"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/28 14:21:39 WARN Utils: Your hostname, Predator-G3572 resolves to a loopback address: 127.0.1.1; using 172.29.43.74 instead (on interface eth0)\n",
      "22/03/28 14:21:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/28 14:21:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/03/28 14:21:41 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "##### add config\n",
    "sc = SparkContext()\n",
    "ssc = StreamingContext(sc, 5)\n",
    "ssc.checkpoint('checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CdFYg6Fqpg4v"
   },
   "outputs": [],
   "source": [
    "int_values = ssc.socketTextStream('localhost', 7000)\n",
    "# open cmd and type: nc -lk 7000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "xfy_S7KHpg4w"
   },
   "outputs": [],
   "source": [
    "# Function adds new values with previous running count to get new count\n",
    "\n",
    "# The update function will be called for each word, with newValues having\n",
    "# a sequence of 1â€™s (from the (word, 1) pairs) and the runningCount having the previous count.\n",
    "\n",
    "def updateFunction(newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "        runningCount = 0\n",
    "    \n",
    "    return sum(newValues, runningCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "aUG6Zzz3pg4w"
   },
   "outputs": [],
   "source": [
    "# Take a remainder value by 10 at each window for the incoming numbers.\n",
    "# Counts how many times each number between 0 and 9 is seen.\n",
    "\n",
    "Remainder = int_values.map(lambda x: int(x) % 10).map(lambda x: (x,1)).updateStateByKey(updateFunction)\n",
    "\n",
    "Remainder.pprint()\n",
    "\n",
    "#Remainder.count().pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bsVWnX58pg4x"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-03-28 14:23:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-03-28 14:23:10\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/28 14:23:15 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/03/28 14:23:15 WARN BlockManager: Block input-0-1648473795200 replicated to only 0 peer(s) instead of 1 peers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-03-28 14:23:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-03-28 14:23:15\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/28 14:23:18 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/03/28 14:23:18 WARN BlockManager: Block input-0-1648473798400 replicated to only 0 peer(s) instead of 1 peers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-03-28 14:23:20\n",
      "-------------------------------------------\n",
      "(0, 1)\n",
      "(2, 1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/28 14:23:21 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/03/28 14:23:21 WARN BlockManager: Block input-0-1648473800800 replicated to only 0 peer(s) instead of 1 peers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-03-28 14:23:20\n",
      "-------------------------------------------\n",
      "(0, 1)\n",
      "(2, 1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-03-28 14:23:25\n",
      "-------------------------------------------\n",
      "(0, 2)\n",
      "(2, 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-03-28 14:23:25\n",
      "-------------------------------------------\n",
      "(0, 2)\n",
      "(2, 1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-03-28 14:23:30\n",
      "-------------------------------------------\n",
      "(0, 2)\n",
      "(2, 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-03-28 14:23:30\n",
      "-------------------------------------------\n",
      "(0, 2)\n",
      "(2, 1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-03-28 14:23:35\n",
      "-------------------------------------------\n",
      "(0, 2)\n",
      "(2, 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-03-28 14:23:35\n",
      "-------------------------------------------\n",
      "(0, 2)\n",
      "(2, 1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-03-28 14:23:40\n",
      "-------------------------------------------\n",
      "(0, 2)\n",
      "(2, 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-03-28 14:23:40\n",
      "-------------------------------------------\n",
      "(0, 2)\n",
      "(2, 1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-03-28 14:23:45\n",
      "-------------------------------------------\n",
      "(0, 2)\n",
      "(2, 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-03-28 14:23:45\n",
      "-------------------------------------------\n",
      "(0, 2)\n",
      "(2, 1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-03-28 14:23:50\n",
      "-------------------------------------------\n",
      "(0, 2)\n",
      "(2, 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-03-28 14:23:50\n",
      "-------------------------------------------\n",
      "(0, 2)\n",
      "(2, 1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-03-28 14:23:55\n",
      "-------------------------------------------\n",
      "(0, 2)\n",
      "(2, 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-03-28 14:23:55\n",
      "-------------------------------------------\n",
      "(0, 2)\n",
      "(2, 1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-03-28 14:24:00\n",
      "-------------------------------------------\n",
      "(0, 2)\n",
      "(2, 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-03-28 14:24:00\n",
      "-------------------------------------------\n",
      "(0, 2)\n",
      "(2, 1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-03-28 14:24:05\n",
      "-------------------------------------------\n",
      "(0, 2)\n",
      "(2, 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-03-28 14:24:05\n",
      "-------------------------------------------\n",
      "(0, 2)\n",
      "(2, 1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "53uTaq-epg4x",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/28 14:24:09 WARN SocketReceiver: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.net.SocketInputStream.socketRead(SocketInputStream.java:116)\n",
      "\tat java.net.SocketInputStream.read(SocketInputStream.java:171)\n",
      "\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\n",
      "\tat sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n",
      "\tat sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n",
      "\tat sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n",
      "\tat java.io.InputStreamReader.read(InputStreamReader.java:184)\n",
      "\tat java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.io.BufferedReader.readLine(BufferedReader.java:324)\n",
      "\tat java.io.BufferedReader.readLine(BufferedReader.java:389)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "22/03/28 14:24:09 ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver\n",
      "22/03/28 14:24:09 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.net.SocketInputStream.socketRead(SocketInputStream.java:116)\n",
      "\tat java.net.SocketInputStream.read(SocketInputStream.java:171)\n",
      "\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\n",
      "\tat sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n",
      "\tat sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n",
      "\tat sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n",
      "\tat java.io.InputStreamReader.read(InputStreamReader.java:184)\n",
      "\tat java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.io.BufferedReader.readLine(BufferedReader.java:324)\n",
      "\tat java.io.BufferedReader.readLine(BufferedReader.java:389)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "22/03/28 14:24:09 WARN ReceiverSupervisorImpl: Receiver has been stopped\n",
      "Exception in thread \"receiver-supervisor-future-0\" java.lang.Error: java.lang.InterruptedException: sleep interrupted\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1155)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.InterruptedException: sleep interrupted\n",
      "\tat java.lang.Thread.sleep(Native Method)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:196)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\t... 2 more\n",
      "22/03/28 14:24:09 WARN BatchedWriteAheadLog: BatchedWriteAheadLog Writer queue interrupted.\n",
      "22/03/28 14:24:10 WARN ReceivedBlockTracker: Exception thrown while writing record: BatchAllocationEvent(1648473850000 ms,AllocatedBlocks(Map(0 -> ArrayBuffer()))) to the WriteAheadLog.\n",
      "java.lang.IllegalStateException: close() was called on BatchedWriteAheadLog before write request with time 1648473850000 could be fulfilled.\n",
      "\tat org.apache.spark.streaming.util.BatchedWriteAheadLog.write(BatchedWriteAheadLog.scala:88)\n",
      "\tat org.apache.spark.streaming.scheduler.ReceivedBlockTracker.writeToLog(ReceivedBlockTracker.scala:244)\n",
      "\tat org.apache.spark.streaming.scheduler.ReceivedBlockTracker.allocateBlocksToBatch(ReceivedBlockTracker.scala:123)\n",
      "\tat org.apache.spark.streaming.scheduler.ReceiverTracker.allocateBlocksToBatch(ReceiverTracker.scala:208)\n",
      "\tat org.apache.spark.streaming.scheduler.JobGenerator.$anonfun$generateJobs$1(JobGenerator.scala:251)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:250)\n",
      "\tat org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:186)\n",
      "\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:91)\n",
      "\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:90)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-03-28 14:24:10\n",
      "-------------------------------------------\n",
      "(0, 2)\n",
      "(2, 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-03-28 14:24:10\n",
      "-------------------------------------------\n",
      "(0, 2)\n",
      "(2, 1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/28 14:24:10 WARN ReceivedBlockTracker: Exception thrown while writing record: BatchCleanupEvent(Vector(1648473825000 ms)) to the WriteAheadLog.\n",
      "java.lang.IllegalStateException: close() was called on BatchedWriteAheadLog before write request with time 1648473850820 could be fulfilled.\n",
      "\tat org.apache.spark.streaming.util.BatchedWriteAheadLog.write(BatchedWriteAheadLog.scala:88)\n",
      "\tat org.apache.spark.streaming.scheduler.ReceivedBlockTracker.writeToLog(ReceivedBlockTracker.scala:244)\n",
      "\tat org.apache.spark.streaming.scheduler.ReceivedBlockTracker.cleanupOldBatches(ReceivedBlockTracker.scala:177)\n",
      "\tat org.apache.spark.streaming.scheduler.ReceiverTracker.cleanupOldBlocksAndBatches(ReceiverTracker.scala:228)\n",
      "\tat org.apache.spark.streaming.scheduler.JobGenerator.clearCheckpointData(JobGenerator.scala:290)\n",
      "\tat org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:190)\n",
      "\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:91)\n",
      "\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:90)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "22/03/28 14:24:10 WARN ReceivedBlockTracker: Failed to acknowledge batch clean up in the Write Ahead Log.\n",
      "22/03/28 14:24:15 WARN ReceivedBlockTracker: Exception thrown while writing record: BatchAllocationEvent(1648473855000 ms,AllocatedBlocks(Map(0 -> ArrayBuffer()))) to the WriteAheadLog.\n",
      "java.lang.IllegalStateException: close() was called on BatchedWriteAheadLog before write request with time 1648473855001 could be fulfilled.\n",
      "\tat org.apache.spark.streaming.util.BatchedWriteAheadLog.write(BatchedWriteAheadLog.scala:88)\n",
      "\tat org.apache.spark.streaming.scheduler.ReceivedBlockTracker.writeToLog(ReceivedBlockTracker.scala:244)\n",
      "\tat org.apache.spark.streaming.scheduler.ReceivedBlockTracker.allocateBlocksToBatch(ReceivedBlockTracker.scala:123)\n",
      "\tat org.apache.spark.streaming.scheduler.ReceiverTracker.allocateBlocksToBatch(ReceiverTracker.scala:208)\n",
      "\tat org.apache.spark.streaming.scheduler.JobGenerator.$anonfun$generateJobs$1(JobGenerator.scala:251)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:250)\n",
      "\tat org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:186)\n",
      "\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:91)\n",
      "\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:90)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-03-28 14:24:15\n",
      "-------------------------------------------\n",
      "(0, 2)\n",
      "(2, 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-03-28 14:24:15\n",
      "-------------------------------------------\n",
      "(0, 2)\n",
      "(2, 1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/28 14:24:15 WARN ReceivedBlockTracker: Exception thrown while writing record: BatchCleanupEvent(Vector(1648473830000 ms, 1648473825000 ms)) to the WriteAheadLog.\n",
      "java.lang.IllegalStateException: close() was called on BatchedWriteAheadLog before write request with time 1648473855841 could be fulfilled.\n",
      "\tat org.apache.spark.streaming.util.BatchedWriteAheadLog.write(BatchedWriteAheadLog.scala:88)\n",
      "\tat org.apache.spark.streaming.scheduler.ReceivedBlockTracker.writeToLog(ReceivedBlockTracker.scala:244)\n",
      "\tat org.apache.spark.streaming.scheduler.ReceivedBlockTracker.cleanupOldBatches(ReceivedBlockTracker.scala:177)\n",
      "\tat org.apache.spark.streaming.scheduler.ReceiverTracker.cleanupOldBlocksAndBatches(ReceiverTracker.scala:228)\n",
      "\tat org.apache.spark.streaming.scheduler.JobGenerator.clearCheckpointData(JobGenerator.scala:290)\n",
      "\tat org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:190)\n",
      "\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:91)\n",
      "\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:90)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "22/03/28 14:24:15 WARN ReceivedBlockTracker: Failed to acknowledge batch clean up in the Write Ahead Log.\n"
     ]
    }
   ],
   "source": [
    "ssc.stop(stopSparkContext=True, stopGraceFully=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "9- Stateful_Transformation_stream.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
